{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocal-religious",
   "metadata": {},
   "source": [
    "# Quiz/Hands-on: Analyzing airline tweets using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portable-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contemporary-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/TL/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-trader",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beautiful-packaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>handle</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825000</td>\n",
       "      <td>American</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>2/22/15 12:01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355000</td>\n",
       "      <td>American</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398000</td>\n",
       "      <td>American</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634000</td>\n",
       "      <td>American</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866000</td>\n",
       "      <td>American</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>2/22/15 11:58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id         airline           handle  \\\n",
       "0      570306133677760000  Virgin America          cairdin   \n",
       "1      570301130888122000  Virgin America         jnardino   \n",
       "2      570301083672813000  Virgin America       yvonnalynn   \n",
       "3      570301031407624000  Virgin America         jnardino   \n",
       "4      570300817074462000  Virgin America         jnardino   \n",
       "...                   ...             ...              ...   \n",
       "14635  569587686496825000        American  KristenReenders   \n",
       "14636  569587371693355000        American         itsropes   \n",
       "14637  569587242672398000        American         sanyabun   \n",
       "14638  569587188687634000        American       SraJackson   \n",
       "14639  569587140490866000        American        daviddtwu   \n",
       "\n",
       "                                                    text  tweet_created  \\\n",
       "0                    @VirginAmerica What @dhepburn said.  2/24/15 11:35   \n",
       "1      @VirginAmerica plus you've added commercials t...  2/24/15 11:15   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...  2/24/15 11:15   \n",
       "3      @VirginAmerica it's really aggressive to blast...  2/24/15 11:15   \n",
       "4      @VirginAmerica and it's a really big bad thing...  2/24/15 11:14   \n",
       "...                                                  ...            ...   \n",
       "14635  @AmericanAir thank you we got on a different f...  2/22/15 12:01   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...  2/22/15 11:59   \n",
       "14637  @AmericanAir Please bring American Airlines to...  2/22/15 11:59   \n",
       "14638  @AmericanAir you have my money, you change my ...  2/22/15 11:59   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...  2/22/15 11:58   \n",
       "\n",
       "                    user_timezone  \n",
       "0      Eastern Time (US & Canada)  \n",
       "1      Pacific Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4      Pacific Time (US & Canada)  \n",
       "...                           ...  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  \n",
       "\n",
       "[14640 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../resources/airline_tweets.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-column",
   "metadata": {},
   "source": [
    "The dataset consists of tweets on airline experiences.\n",
    "\n",
    "After looking it up online, the dataset seems to be a shortened version of the <a href=\"https://www.kaggle.com/crowdflower/twitter-airline-sentiment\">twitter airline sentiment</a> dataset, available on kaggle or crowdflower.\n",
    "\n",
    "According to the kaggle page,\n",
    "\n",
    "> A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-server",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "For this dataset, I wish to figure out people's sentiments towards the different airlines mentioned in the tweets. I want to know the underlying reasons behind those sentiments. What is it about some airlines that people like or dislike?\n",
    "\n",
    "I chose this as the goal since it has some real life merit. Knowing this may be useful so airlines know which aspects they can improve in, and for passengers to know what they may expect when going with a particular airline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-signal",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "In order to fulfill the goals mentioned above, the following steps will be taken:\n",
    "\n",
    "1. **Prepare the data.**\n",
    "2. **Identify the different airlines mentioned in the dataset.**\n",
    "3. **Group together tweets according to airline and find the sentiment for each group.**\n",
    "4. **Attempt to identify the reasons for the sentiments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-recipe",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Text Cleaning\n",
    "It is best practice to maintain two forms of text, for context-aware and word-level tasks.\n",
    "\n",
    "**Context Aware Tasks**<br>\n",
    "For this, we remove:\n",
    "* Special characters\n",
    "    * Except punctuations at the end of a sentence, which is important for dependency parsing.\n",
    "* leading and trailing whitespaces\n",
    "* Twitter handles\n",
    "* hashtags\n",
    "\n",
    "**Word-level Tasks:**<br>\n",
    "Same as above, but additionally: \n",
    "* lowercased\n",
    "* lemmatized\n",
    "* stopword-removed\n",
    "* special characters removed\n",
    "\n",
    "We also keep the text in its original form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-fighter",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aging-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextCleaner:\n",
    "    \"\"\"Cleaning functions.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop = set(stopwords.words(\"english\"))\n",
    "        \n",
    "    def clean_context_aware(self, text: str) -> str:\n",
    "        \"\"\"Clean text for context-level analysis.\"\"\"\n",
    "\n",
    "        # Remove the ff:\n",
    "        # Twitter handles and hashtags\n",
    "        text = re.sub(\"[#@][\\w]+\", \"\", text)\n",
    "\n",
    "        # Special characters (except sentence endings)\n",
    "        text = re.sub(\"[^\\w?!\\.,\\s]+\", \"\", text)\n",
    "\n",
    "        # Extra spaces\n",
    "        text = re.sub(\"\\s+\", \" \", text)  # Multiple consecutive spaces\n",
    "        text = re.sub(\"\\s\\.\", \".\", text)  # Space before period\n",
    "        text = text.strip()  # Leading and trailing whitespaces\n",
    "\n",
    "        return text\n",
    "        \n",
    "    def clean_word_level(self, text: str) -> str:\n",
    "        \"\"\"Clean text for word-level analysis.\"\"\"\n",
    "        text = self.clean_context_aware(text)\n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "        # remove all special chars\n",
    "        text = re.sub(\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "        # remove stopwords and lemmatize\n",
    "        tokens = text.split()\n",
    "        tokens = [t for t in tokens if t not in self.stop]\n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "answering-birthday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_context</th>\n",
       "      <th>text_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>What said.</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus youve added commercials to the experience...</td>\n",
       "      <td>plus youve added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>I didnt today... Must mean I need to take anot...</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>its really aggressive to blast obnoxious enter...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>thank you we got on a different flight to Chic...</td>\n",
       "      <td>thank got different flight chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>leaving over 20 minutes Late Flight. No warnin...</td>\n",
       "      <td>leaving 20 minute late flight warning communic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>Please bring American Airlines to</td>\n",
       "      <td>please bring american airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>you have my money, you change my flight, and d...</td>\n",
       "      <td>money change flight dont answer phone suggesti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>we have 8 ppl so we need 2 know how many seats...</td>\n",
       "      <td>8 ppl need 2 know many seat next flight plz pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                    @VirginAmerica What @dhepburn said.   \n",
       "1      @VirginAmerica plus you've added commercials t...   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...   \n",
       "3      @VirginAmerica it's really aggressive to blast...   \n",
       "4      @VirginAmerica and it's a really big bad thing...   \n",
       "...                                                  ...   \n",
       "14635  @AmericanAir thank you we got on a different f...   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637  @AmericanAir Please bring American Airlines to...   \n",
       "14638  @AmericanAir you have my money, you change my ...   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "                                            text_context  \\\n",
       "0                                             What said.   \n",
       "1      plus youve added commercials to the experience...   \n",
       "2      I didnt today... Must mean I need to take anot...   \n",
       "3      its really aggressive to blast obnoxious enter...   \n",
       "4                and its a really big bad thing about it   \n",
       "...                                                  ...   \n",
       "14635  thank you we got on a different flight to Chic...   \n",
       "14636  leaving over 20 minutes Late Flight. No warnin...   \n",
       "14637                  Please bring American Airlines to   \n",
       "14638  you have my money, you change my flight, and d...   \n",
       "14639  we have 8 ppl so we need 2 know how many seats...   \n",
       "\n",
       "                                               text_word  \n",
       "0                                                   said  \n",
       "1           plus youve added commercial experience tacky  \n",
       "2           didnt today must mean need take another trip  \n",
       "3      really aggressive blast obnoxious entertainmen...  \n",
       "4                                   really big bad thing  \n",
       "...                                                  ...  \n",
       "14635                 thank got different flight chicago  \n",
       "14636  leaving 20 minute late flight warning communic...  \n",
       "14637                      please bring american airline  \n",
       "14638  money change flight dont answer phone suggesti...  \n",
       "14639  8 ppl need 2 know many seat next flight plz pu...  \n",
       "\n",
       "[14640 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner = TextCleaner()\n",
    "df['text_context'] = df['text'].map(cleaner.clean_context_aware)\n",
    "df['text_word'] = df['text'].map(cleaner.clean_word_level)\n",
    "df[['text', 'text_context', 'text_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "polar-documentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>handle</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>text_context</th>\n",
       "      <th>text_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>What said.</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>plus youve added commercials to the experience...</td>\n",
       "      <td>plus youve added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>I didnt today... Must mean I need to take anot...</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>its really aggressive to blast obnoxious enter...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825000</td>\n",
       "      <td>American</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>2/22/15 12:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you we got on a different flight to Chic...</td>\n",
       "      <td>thank got different flight chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355000</td>\n",
       "      <td>American</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaving over 20 minutes Late Flight. No warnin...</td>\n",
       "      <td>leaving 20 minute late flight warning communic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398000</td>\n",
       "      <td>American</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please bring American Airlines to</td>\n",
       "      <td>please bring american airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634000</td>\n",
       "      <td>American</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>2/22/15 11:59</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>you have my money, you change my flight, and d...</td>\n",
       "      <td>money change flight dont answer phone suggesti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866000</td>\n",
       "      <td>American</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>2/22/15 11:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we have 8 ppl so we need 2 know how many seats...</td>\n",
       "      <td>8 ppl need 2 know many seat next flight plz pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id         airline           handle  \\\n",
       "0      570306133677760000  Virgin America          cairdin   \n",
       "1      570301130888122000  Virgin America         jnardino   \n",
       "2      570301083672813000  Virgin America       yvonnalynn   \n",
       "3      570301031407624000  Virgin America         jnardino   \n",
       "4      570300817074462000  Virgin America         jnardino   \n",
       "...                   ...             ...              ...   \n",
       "14635  569587686496825000        American  KristenReenders   \n",
       "14636  569587371693355000        American         itsropes   \n",
       "14637  569587242672398000        American         sanyabun   \n",
       "14638  569587188687634000        American       SraJackson   \n",
       "14639  569587140490866000        American        daviddtwu   \n",
       "\n",
       "                                                    text  tweet_created  \\\n",
       "0                    @VirginAmerica What @dhepburn said.  2/24/15 11:35   \n",
       "1      @VirginAmerica plus you've added commercials t...  2/24/15 11:15   \n",
       "2      @VirginAmerica I didn't today... Must mean I n...  2/24/15 11:15   \n",
       "3      @VirginAmerica it's really aggressive to blast...  2/24/15 11:15   \n",
       "4      @VirginAmerica and it's a really big bad thing...  2/24/15 11:14   \n",
       "...                                                  ...            ...   \n",
       "14635  @AmericanAir thank you we got on a different f...  2/22/15 12:01   \n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...  2/22/15 11:59   \n",
       "14637  @AmericanAir Please bring American Airlines to...  2/22/15 11:59   \n",
       "14638  @AmericanAir you have my money, you change my ...  2/22/15 11:59   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...  2/22/15 11:58   \n",
       "\n",
       "                    user_timezone  \\\n",
       "0      Eastern Time (US & Canada)   \n",
       "1      Pacific Time (US & Canada)   \n",
       "2      Central Time (US & Canada)   \n",
       "3      Pacific Time (US & Canada)   \n",
       "4      Pacific Time (US & Canada)   \n",
       "...                           ...   \n",
       "14635                         NaN   \n",
       "14636                         NaN   \n",
       "14637                         NaN   \n",
       "14638  Eastern Time (US & Canada)   \n",
       "14639                         NaN   \n",
       "\n",
       "                                            text_context  \\\n",
       "0                                             What said.   \n",
       "1      plus youve added commercials to the experience...   \n",
       "2      I didnt today... Must mean I need to take anot...   \n",
       "3      its really aggressive to blast obnoxious enter...   \n",
       "4                and its a really big bad thing about it   \n",
       "...                                                  ...   \n",
       "14635  thank you we got on a different flight to Chic...   \n",
       "14636  leaving over 20 minutes Late Flight. No warnin...   \n",
       "14637                  Please bring American Airlines to   \n",
       "14638  you have my money, you change my flight, and d...   \n",
       "14639  we have 8 ppl so we need 2 know how many seats...   \n",
       "\n",
       "                                               text_word  \n",
       "0                                                   said  \n",
       "1           plus youve added commercial experience tacky  \n",
       "2           didnt today must mean need take another trip  \n",
       "3      really aggressive blast obnoxious entertainmen...  \n",
       "4                                   really big bad thing  \n",
       "...                                                  ...  \n",
       "14635                 thank got different flight chicago  \n",
       "14636  leaving 20 minute late flight warning communic...  \n",
       "14637                      please bring american airline  \n",
       "14638  money change flight dont answer phone suggesti...  \n",
       "14639  8 ppl need 2 know many seat next flight plz pu...  \n",
       "\n",
       "[14640 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_aware_df = df.copy()\n",
    "context_aware_df['text'].map(cleaner.clean_context_aware)\n",
    "context_aware_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "healthy-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()\n",
    "context_aware_df = df.copy()\n",
    "context_aware_df['text']\n",
    "word_level_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-samba",
   "metadata": {},
   "source": [
    "# Data Exploration / Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-canberra",
   "metadata": {},
   "source": [
    "## Identifying Airlines\n",
    "\n",
    "I know that the data is composed of tweets on different airlines. However, I do not know which specific airlines these are. I will try to find this out in the next section by checking the twitter handles mentioned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get twitter handles and the number of mentions\n",
    "twitter_handles_freq = {}\n",
    "tw_handle_regex = re.compile(\"@[\\w]+\")\n",
    "\n",
    "for text in df['text']:\n",
    "    for handle in tw_handle_regex.findall(text):\n",
    "        handle = handle.lower()\n",
    "        twitter_handles_freq[handle] = twitter_handles_freq.get(handle, 0) + 1\n",
    "        \n",
    "# To get only airlines\n",
    "# Remove handles mentioned < 100 times\n",
    "airlines = {k:v for k, v in twitter_handles_freq.items() if v >= 100}\n",
    "airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-florence",
   "metadata": {},
   "source": [
    "We then create a frequency graph to see the distribution of airline tweets in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4), dpi=90)\n",
    "x = sorted(airlines.keys(), reverse=True)\n",
    "y = sorted(airlines.values(), reverse=True)\n",
    "ax.bar(x, y)\n",
    "ax.set_title(\"Airline Tweet Distribution\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-million",
   "metadata": {},
   "source": [
    "It seems that there are 6 main airlines featured in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-turner",
   "metadata": {},
   "source": [
    "## Date Distribution\n",
    "\n",
    "According to the original source, the tweets were taken from a time interval in February 2015. I want to confirm this, and further see the distribution of tweets in that month. I also want to see if there were any significant events that happened in that time period which may affect the contents of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df['tweet_created'].groupby(by=[df[\"tweet_created\"].dt.month, df[\"tweet_created\"].dt.day]).count()\n",
    "fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "ax.bar([i[1] for i in d.index], d.values)\n",
    "ax.set_title(\"Tweet Date Distribution\")\n",
    "ax.set_xlabel(\"Day (Feb, 2015)\")\n",
    "ax.set_ylabel(\"Number of Tweets\")\n",
    "ax.set_xticks(range(16, 25))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-audience",
   "metadata": {},
   "source": [
    "The tweets are from 16-24 of February, 2015. On Feb 22 and 23, there is a notable spike in the number of tweets. Aside from Chinese New Years on Feb 19, there weren't any holidays or notable news events at this time. \n",
    "\n",
    "The spike in the number of tweets might be explained by the fact that 21 and 22 are weekends (Sat, Sun). A greater number of people may have traveled during the weekends (21, 22) and then posted about their experience the day after (22, 23)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-knife",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We group together the different tweets according to which of 6 main airlines they mention, and perform sentiment analysis on each group.\n",
    "\n",
    "We use `SentimentIntensityAnalyzer`, which takes in full sentences. We use the original, uncleaned text for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VADER model\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['text', 'text_context', 'text_word']\n",
    "text_cols + ['neg', 'neu', 'pos', 'compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['text', 'text_context', 'text_word']\n",
    "sentiment_df = pd.DataFrame(columns = text_cols + ['neg', 'neu', 'pos', 'compound'])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    res = sia.polarity_scores(row['text'])\n",
    "    text_values = {col: row[col] for col in text_cols}\n",
    "    sentiment_df = sentiment_df.append({**text_values, **res}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(columns=['text', 'text_context', 'text_word', 'neg', 'neu', 'pos', 'compound'])\n",
    "\n",
    "for text in df['text']:\n",
    "    res = sia.polarity_scores(text)\n",
    "    sentiment_df = sentiment_df.append({'text': text, **res}, ignore_index=True)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-slovenia",
   "metadata": {},
   "source": [
    "## Average sentiment for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100)\n",
    "ave_sentiment_scores_all = sentiment_df[['neg', 'neu', 'pos']].sum(axis=0) / sentiment_df.shape[0]\n",
    "ave_sentiment_scores_all.plot.bar(title='Sentiment, All Tweets', ylabel='# of tweets', figsize=(4, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-consortium",
   "metadata": {},
   "source": [
    "## Ave sentiment for each airline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_airline['@virginamerica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the tweets together according to airline\n",
    "# Note: This approach isn't perfect\n",
    "# It may fail to classify correctly a tweet which mentions multiple airlines\n",
    "\n",
    "tweets_by_airline = {k:pd.DataFrame(columns=sentiment_df.columns) for k in airlines.keys()}\n",
    "\n",
    "for i, row in sentiment_df.iterrows():\n",
    "    tweet = row['text']\n",
    "    for airline in airlines:\n",
    "        if airline in tweet.lower():\n",
    "            tweets_by_airline[airline] = tweets_by_airline[airline].append(row)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-imperial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airline_scores_ave = {}\n",
    "for airline, df_ in tweets_by_airline.items():\n",
    "    scores = df_[['neg', 'neu', 'pos']]\n",
    "    airline_scores_ave[airline] = scores.sum(axis=0) / df_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(8, 8), dpi=100)\n",
    "for airline_scores, ax in list(zip(airline_scores_ave.items(), fig.axes)):\n",
    "    airline, scores = airline_scores\n",
    "    ax.set_title(airline)\n",
    "    \n",
    "    d = dict(scores)\n",
    "    ax.bar(*zip(*d.items()))\n",
    "\n",
    "fig.suptitle(\"Sentiment by Airline\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-insight",
   "metadata": {},
   "source": [
    "Surprisingly enough, the sentiment for each airline is generally more positive than negative (with the sole exception being for US Airways. Given the amount of airline-related horror stories I've heard, I would have expected more negative sentiment from the tweets.\n",
    "\n",
    "Now that I know the general sentiment for each airline, I want to know the reason for those sentiments. What is it that people usually like/hate about their experience? We find this out by creating word clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-majority",
   "metadata": {},
   "source": [
    "## Airline Rankings\n",
    "\n",
    "Airlines ranked by compound sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "\n",
    "airlines_sorted = sorted(list(ave_compound_scores.items()), key=lambda x: x[1])\n",
    "\n",
    "# Example data\n",
    "airlines = [a[0] for a in airlines_sorted]\n",
    "y_pos = range(len(airlines_sorted))\n",
    "performance = [a[1] for a in airlines_sorted]\n",
    "ax.barh(y_pos, performance,  align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(airlines)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('compound score')\n",
    "ax.set_title('Airline Sentiments')\n",
    "plt.axvline(x=0, c='black', lw=1, ls='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-slope",
   "metadata": {},
   "source": [
    "* US Airways is the most disliked - the only airline with a negative score.\n",
    "* American Air also has a relatively low score.\n",
    "* United Airlines is by far the most liked, with more than twice the score of the 2nd placed Virgin America."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-machine",
   "metadata": {},
   "source": [
    "## Identifying the Reason for the Sentiment Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-transparency",
   "metadata": {},
   "source": [
    "To achieve the goal for this section, we will use Word Clouds and Topic Modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install seaborn wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(ax, tweets: Iterable[str], max_words: int=500, title: str=\"\"):\n",
    "    \"\"\"Create a wordcloud of most common words in a set of tweets\"\"\"\n",
    "    \n",
    "    # Transform text for WordCloud\n",
    "    tweets = ' '.join(tweets)\n",
    "    tweets = tweets.replace(' ', ',')\n",
    "    \n",
    "    # Generate wordcloud image\n",
    "    wc = WordCloud(width=1200, height=1200, background_color=\"white\", \n",
    "                   max_words=max_words)\n",
    "    wc.generate(tweets)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(title, size=20)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-aside",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12), dpi=200)\n",
    "for airline, ax in zip(tweets_by_airline.keys(), fig.axes):\n",
    "    create_wordcloud(ax, tweets_by_airline[airline]['text_word'], max_words=200, title=airline)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-heater",
   "metadata": {},
   "source": [
    "So far, there really isn't anything unexpected from these word clouds. We see common air travel terms such as `flight`, `delay`, `cancelled`, etc.\n",
    "\n",
    "The word `thank` shows up a lot. This supports the sentiment analysis results, which show higher positive sentiment. It seems that people like tweeting about their positive experiences and then thanking the airline. Another reason that it shows up so much is that peope usually end a question with \"thanks!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in sentiment_df[sentiment_df['text_word'].map(lambda x: 'thank' in x)]['text'].iloc[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-action",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "To fulfill the same purpose as the word cloud earlier, we also create word frequency graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = {}\n",
    "for i, row in df.iterrows():\n",
    "    for word in row['text_word'].split():\n",
    "        word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "        \n",
    "# Get sorted list\n",
    "word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "# Keep only top 10 words\n",
    "word_frequency = word_frequency[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency\n",
    "keys, items = zip(*word_frequency)\n",
    "print(keys)\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, items = zip(*word_frequency)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4), dpi=100)\n",
    "ax.bar(keys, items)\n",
    "ax.set_title(\"Word Frequency, All\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-journalist",
   "metadata": {},
   "source": [
    "Again, there seems to be nothing unusual here. It is natural that most tweets on airlines will mention a flight.\n",
    "The 6th most mentioned word `u` is just a shortening of `you`.\n",
    "\n",
    "So far, I still haven't fulfilled my goal of finding out the reasons why people might like or dislike a particular airline. To achieve this, it might be useful to group tweets into either positive or negative, and then get the word frequencies for each group. For this, we will say that any tweets with `compound > 0` is positive, and `compound < 0` is negative. Those with `compound == 0` are considered neutral and will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "\n",
    "for i, row in sentiment_df.iterrows():\n",
    "    if row['compound'] > 0:\n",
    "        positive_tweets.append(row['text_word'])\n",
    "    elif row['compound'] < 0:\n",
    "        negative_tweets.append(row['text_word'])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_list(corpus: List['str']) -> \"word freq dict\":\n",
    "    word_freq = {}\n",
    "    for text in corpus:\n",
    "        for word in text.split():\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    return word_freq\n",
    "\n",
    "pos_words_freq = word_count_list(positive_tweets)\n",
    "neg_words_freq = word_count_list(negative_tweets)\n",
    "\n",
    "# Keep only the top 10 words\n",
    "pos_words_freq = sorted(pos_words_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "neg_words_freq = sorted(neg_words_freq.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse lists again for horizontal bar graph\n",
    "word_freq_lists = [pos_words_freq[::-1], neg_words_freq[::-1]]\n",
    "titles = [\"Positive Tweets Word Frequency\", \"Negative Tweets Word Frequency\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), dpi=100)\n",
    "\n",
    "for ax, word_freq_dict, title in zip(fig.axes, word_freq_lists, titles):\n",
    "    \n",
    "    keys, values = zip(*word_freq_dict)\n",
    "    ax.barh(keys, values)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-democracy",
   "metadata": {},
   "source": [
    "#### Positive Tweets\n",
    "The positive tweets are generally those thanking the airline for a good experience, or for good customer service.\n",
    "\n",
    "#### Negative Tweets\n",
    "Negative tweets are generally about cancelled or delayed flights.\n",
    "\n",
    "A very unusual word is `flightled`. It happens too often to be a typo, and it is found on tweets for Virgin America. I could find no information on it online, especially since Virgin America Airlines does not exist anymore. I have to assume it is some kind of flight deal or business parlance that Virgin America used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightled = df[df['text_word'].map(lambda x: \"flightled\" in x)]\n",
    "for text in flightled['text'].iloc[:10]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-great",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis, pyLDAvis.gensim_models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_texts = []\n",
    "\n",
    "for text in df['text_context'].iloc[:5]:\n",
    "    doc = nlp(text)\n",
    "    split_text = [token.text for token in doc]\n",
    "    split_texts.append(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-concentrate",
   "metadata": {},
   "source": [
    "# Findings, Summary\n",
    "* The dataset contains tweets towards 6 major airlines: **US Airways, American Air, Jet Blue, Southwest Air, Virgin America, & United**.\n",
    "* There are more positive tweets than negative tweets, in the entire dataset.\n",
    "* On a per-airline basis, **United** has the best average sentiment score, and **US Airways** has the worst -- being the only one with a negative average compound sentiment.\n",
    "* Positive tweets are generally those thanking the airline for a good experience or for good customer service.\n",
    "* Negative tweets are generally those complaining about cancelled or delayed flights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-health",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
