{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BiKR78k2k-xF",
   "metadata": {
    "id": "BiKR78k2k-xF"
   },
   "source": [
    "# Natural Language Processing: Context-aware Tasks\n",
    "\n",
    "Hi everyone! Today, we're continuing with NLP, specifically looking at context-aware problems, features that we can extract at a document-level, and common context-aware tasks such as rules-based sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jAyu5SR9k-xG",
   "metadata": {
    "id": "jAyu5SR9k-xG"
   },
   "source": [
    "We will be extensively using SpaCy's `en_core_web_lg model` for many of the context-aware tasks today. This model was pre-trained on several corpuses such as OntoNotes 5, GloVe Common Crawl, and others ([source](https://spacy.io/models/en])). It comes with pipeline components such as a tokenizer, a POS tagger, a dependency parse, a lemmatizer, and an named entity recognizer. It also comes with pre-trained document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ERp2NAZzk-xG",
   "metadata": {
    "id": "ERp2NAZzk-xG"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='spacy_pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-m0QOaJXk-xH",
   "metadata": {
    "id": "-m0QOaJXk-xH"
   },
   "source": [
    "By default, the SpaCy pipeline's preprocessing steps include tokenization, POS tagging, dependency parsing, and named entity recognition only. However, users have the option to use the other features such as lemmatization, embedding, and other self-defined steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dNLyvwxk-xH",
   "metadata": {
    "id": "4dNLyvwxk-xH"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run only if you do not have SpaCy and the en_core_web_lg model installed on your device yet\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxGLeCyxk-xH",
   "metadata": {
    "id": "sxGLeCyxk-xH"
   },
   "source": [
    "### Parts-of-speech (POS) tagging and dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zMsfimrOk-xH",
   "metadata": {
    "id": "zMsfimrOk-xH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BmAq17V-k-xI",
   "metadata": {
    "id": "BmAq17V-k-xI"
   },
   "outputs": [],
   "source": [
    "# Import spacy and the en_core_web_lg model\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except:\n",
    "    print(\"Error loading 'en_core_web_lg' model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0HPjvHgk-xI",
   "metadata": {
    "id": "r0HPjvHgk-xI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the en_core_web_lg_model to pre-process our text\n",
    "SpaCy's pre-trained model automatically determines various linguistic properties\n",
    "such as POS tags and dependency trees\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"The Philippine flight was delayed due to trouble with the airplane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HyIvDdZqk-xI",
   "metadata": {
    "id": "HyIvDdZqk-xI"
   },
   "outputs": [],
   "source": [
    "# Visualize every \"token\" in the document\n",
    "tokens = pd.DataFrame(columns=\n",
    "                      ['text', 'lemma', 'pos', 'tag',\n",
    "                       'dependency', 'shape', 'is_alphabet',\n",
    "                       'is_stopword', 'head_text', 'head_pos'])\n",
    "\n",
    "for token in doc:\n",
    "    data = [token.text, token.lemma_, token.pos_,\n",
    "            token.tag_, token.dep_, token.shape_,\n",
    "            token.is_alpha, token.is_stop,\n",
    "            token.head.text, token.head.pos_]\n",
    "    tokens.loc[len(tokens)] = data\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6LEWLj9rk-xI",
   "metadata": {
    "id": "6LEWLj9rk-xI"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_bL36cIGk-xI",
   "metadata": {
    "id": "_bL36cIGk-xI"
   },
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XG0ZpwCPk-xJ",
   "metadata": {
    "id": "XG0ZpwCPk-xJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the en_core_web_lg_model to pre-process our text\n",
    "SpaCy's pre-trained model also determines any named entities from text. \n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"The Philippine flight was delayed yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IP_JeOCdk-xJ",
   "metadata": {
    "id": "IP_JeOCdk-xJ"
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ktcZKj1k-xJ",
   "metadata": {
    "id": "3ktcZKj1k-xJ"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WIY8arb1k-xJ",
   "metadata": {
    "id": "WIY8arb1k-xJ"
   },
   "source": [
    "### Rules-based sentiment analysis\n",
    "\n",
    "For sentiment analysis, we will be using the VADER model ([source](https://github.com/cjhutto/vaderSentiment)). VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "\n",
    "We will modify the SpaCy pipeline to add the additional step of using the VADER model to calculate the sentiment polarity of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J30hyqNFk-xK",
   "metadata": {
    "id": "J30hyqNFk-xK"
   },
   "outputs": [],
   "source": [
    "# # Uncomment and run only if you do not have VADER installed on your device yet\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cM0QeQHek-xK",
   "metadata": {
    "id": "cM0QeQHek-xK"
   },
   "outputs": [],
   "source": [
    "# Import VADER model\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Import Doc for extending the SpaCy pipeline\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zLR_kKaLk-xK",
   "metadata": {
    "id": "zLR_kKaLk-xK"
   },
   "outputs": [],
   "source": [
    "# Define sentiment analysis extensions\n",
    "\"\"\"\n",
    "You can define the extension as a regular Python function:\n",
    "def sentiment_analysis(doc):\n",
    "    return sia.polarity_scores(doc.text)\n",
    "\"\"\"\n",
    "# Or you can create an anonymous lambda functions\n",
    "sentiment_analysis = lambda doc: sia.polarity_scores(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hb42zhi2k-xK",
   "metadata": {
    "id": "hb42zhi2k-xK"
   },
   "outputs": [],
   "source": [
    "# Instantiate NLP pipeline and set extensions\n",
    "nlp_with_sentiment = spacy.load(\"en_core_web_lg\")\n",
    "Doc.set_extension(\"sentiment\", getter=sentiment_analysis, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wL8_UDrxk-xK",
   "metadata": {
    "id": "wL8_UDrxk-xK"
   },
   "outputs": [],
   "source": [
    "doc = nlp_with_sentiment(\"I love this school, but it's tiring sometimes!\")\n",
    "doc._.sentiment # Acccess custom properties using the \"._.\" operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBKM44Aqk-xK",
   "metadata": {
    "id": "JBKM44Aqk-xK"
   },
   "source": [
    "### Process multiple documents\n",
    "\n",
    "SpaCy pipelines have an optimazation for processing multiple texts all at once using the `nlp.pipe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jlwp4XCRk-xK",
   "metadata": {
    "id": "jlwp4XCRk-xK"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I love this school, but it's tiring sometimes!\",\n",
    "        \"I'm looking forward to Christmas break.\",\n",
    "        \"I'm sad that midterms are over.\",\n",
    "        \"School is almost over.\",\n",
    "        \"I'll be relaxing during the Christmas break.\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GiL-lN4lk-xL",
   "metadata": {
    "id": "GiL-lN4lk-xL"
   },
   "outputs": [],
   "source": [
    "# Use the nlp.pipe() method to process a collection of texts\n",
    "docs = nlp_with_sentiment.pipe(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Opro5uXUk-xL",
   "metadata": {
    "id": "Opro5uXUk-xL"
   },
   "outputs": [],
   "source": [
    "df_modified = pd.DataFrame(columns=['text', 'document', 'com_sent', 'pos_sent', 'neg_sent', 'neu_sent'])\n",
    "for doc in docs:\n",
    "    df_modified = df_modified.append(\n",
    "        {\n",
    "            'text': doc.text,\n",
    "            'document': doc,\n",
    "            'com_sent': doc._.sentiment['compound'],\n",
    "            'pos_sent': doc._.sentiment['pos'],\n",
    "            'neg_sent': doc._.sentiment['neg'],\n",
    "            'neu_sent': doc._.sentiment['neu']\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "df_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Er9ZOz9Mk-xL",
   "metadata": {
    "id": "Er9ZOz9Mk-xL"
   },
   "source": [
    "### Document embeddings\n",
    "\n",
    "SpaCy's `en_core_web_lg` model also comes with pre-trained document embeddings. As such, we can simply use them to immediately retrieve the embeddings of our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6H0YZ5vk-xL",
   "metadata": {
    "id": "p6H0YZ5vk-xL"
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vlRud-2Xk-xL",
   "metadata": {
    "id": "vlRud-2Xk-xL"
   },
   "outputs": [],
   "source": [
    "doc.vector.shape # The embedding uses 300 feature columns only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qbCv9PK9k-xL",
   "metadata": {
    "id": "qbCv9PK9k-xL"
   },
   "outputs": [],
   "source": [
    "doc.vector # This is the embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dk5tX8sAk-xM",
   "metadata": {
    "id": "dk5tX8sAk-xM"
   },
   "source": [
    "In the next following cells, we will attempt to embed a bunch of documents to retrieve their embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NUW2g0dEk-xM",
   "metadata": {
    "id": "NUW2g0dEk-xM"
   },
   "outputs": [],
   "source": [
    "df_embedded = df.copy()\n",
    "df_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-cNhv_ovk-xM",
   "metadata": {
    "id": "-cNhv_ovk-xM"
   },
   "outputs": [],
   "source": [
    "# Embed a bunch of documents and append them to a dataframe\n",
    "documents = []\n",
    "vectors = []\n",
    "\n",
    "for doc in nlp.pipe(df['text']):\n",
    "    documents.append(doc)\n",
    "    vectors.append(doc.vector)\n",
    "    \n",
    "df_embedded['document'] = pd.Series(documents, name='document')\n",
    "df_embedded = df_embedded.join(pd.DataFrame(vectors))\n",
    "df_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NO7G81Ark-xM",
   "metadata": {
    "id": "NO7G81Ark-xM"
   },
   "source": [
    "### Document similarity\n",
    "\n",
    "Now that we have document embeddings, we can use techniques such as cosine similarity to determine similarity between texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XX-yHDiJk-xM",
   "metadata": {
    "id": "XX-yHDiJk-xM"
   },
   "outputs": [],
   "source": [
    "a = nlp(\"Hello there!\")\n",
    "b = nlp(\"Greetings to you!\")\n",
    "c = nlp(\"Wikipedia is not a dictionary, or a usage or jargon guide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lRvv3XvHk-xM",
   "metadata": {
    "id": "lRvv3XvHk-xM"
   },
   "outputs": [],
   "source": [
    "a.similarity(b) # The two sentences look very similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGnxTlH0k-xN",
   "metadata": {
    "id": "lGnxTlH0k-xN"
   },
   "outputs": [],
   "source": [
    "a.similarity(c) # Not so similar (almost 50-50 coin toss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TSsCbYQdk-xN",
   "metadata": {
    "id": "TSsCbYQdk-xN"
   },
   "source": [
    "### Topic modeling and thematic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8GkeeE7Yk-xN",
   "metadata": {
    "id": "8GkeeE7Yk-xN"
   },
   "source": [
    "For topic modeling and thematic analysis, we will be using GenSim ([source](https://radimrehurek.com/gensim/)) and pyLDAvis ([source](https://github.com/bmabey/pyLDAvis)). We will use GenSim to implement the Latent-Dirichlet Allocation (LDA) model, and use pyLDAvis to visualize the results. \n",
    "\n",
    "WARNING: Your notebook might become buggy when using pyLDAvis. Once you are done looking at the visual, simply clear the output of the cell that uses the pyLDAvis chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pO1f8H77k-xN",
   "metadata": {
    "id": "pO1f8H77k-xN"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run only if you do not have gensim and pyLDAvis on your device yet\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rb8jAV6ck-xN",
   "metadata": {
    "id": "Rb8jAV6ck-xN"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis, pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r1RT6CaEk-xN",
   "metadata": {
    "id": "r1RT6CaEk-xN"
   },
   "outputs": [],
   "source": [
    "# Create a corpus from the documents\n",
    "split_texts = [[token.text for token in doc] for doc in df_modified['document']] # Get tokens from each document\n",
    "dictionary = Dictionary(split_texts) # Create a GenSimdictionary\n",
    "corpus = [dictionary.doc2bow(text) for text in split_texts] # Create a GenSim corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XAeeQmZEk-xN",
   "metadata": {
    "id": "XAeeQmZEk-xN"
   },
   "outputs": [],
   "source": [
    "# Create an LDA model with 3 topis from the generated corpus\n",
    "lda = LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=dictionary, \n",
    "    num_topics=3\n",
    ")\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zo_q3Upgk-xO",
   "metadata": {
    "id": "zo_q3Upgk-xO"
   },
   "outputs": [],
   "source": [
    "# Visualize the topic models and figure out the themes\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "vis\n",
    "\n",
    "# Right-click the cell then choose \"Clear outputs\" when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "06_nlp_context_aware.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
