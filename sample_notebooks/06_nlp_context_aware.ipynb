{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BiKR78k2k-xF",
   "metadata": {
    "id": "BiKR78k2k-xF"
   },
   "source": [
    "# Natural Language Processing: Context-aware Tasks\n",
    "\n",
    "Hi everyone! Today, we're continuing with NLP, specifically looking at context-aware problems, features that we can extract at a document-level, and common context-aware tasks such as rules-based sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jAyu5SR9k-xG",
   "metadata": {
    "id": "jAyu5SR9k-xG"
   },
   "source": [
    "We will be extensively using SpaCy's `en_core_web_lg model` for many of the context-aware tasks today. This model was pre-trained on several corpuses such as OntoNotes 5, GloVe Common Crawl, and others ([source](https://spacy.io/models/en])). It comes with pipeline components such as a tokenizer, a POS tagger, a dependency parse, a lemmatizer, and an named entity recognizer. It also comes with pre-trained document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ERp2NAZzk-xG",
   "metadata": {
    "id": "ERp2NAZzk-xG"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spacy_pipeline.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qp/v1yy0hns3vqdxskbv64ynv7c0000gn/T/ipykernel_84564/3551476781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spacy_pipeline.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spacy_pipeline.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='spacy_pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-m0QOaJXk-xH",
   "metadata": {
    "id": "-m0QOaJXk-xH"
   },
   "source": [
    "By default, the SpaCy pipeline's preprocessing steps include tokenization, POS tagging, dependency parsing, and named entity recognition only. However, users have the option to use the other features such as lemmatization, embedding, and other self-defined steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dNLyvwxk-xH",
   "metadata": {
    "id": "4dNLyvwxk-xH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (3.1.3)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.2.0-cp38-cp38-macosx_10_9_x86_64.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 224 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (1.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: jinja2 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 433 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-macosx_10_9_x86_64.whl (609 kB)\n",
      "\u001b[K     |████████████████████████████████| 609 kB 301 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: setuptools in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (47.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/TL/.pyenv/versions/3.8.5/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: thinc, spacy-loggers, langcodes, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.10\n",
      "    Uninstalling thinc-8.0.10:\n",
      "      Successfully uninstalled thinc-8.0.10\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.1.3\n",
      "    Uninstalling spacy-3.1.3:\n",
      "      Successfully uninstalled spacy-3.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 3.2.0 which is incompatible.\n",
      "en-core-web-lg 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed langcodes-3.3.0 spacy-3.2.0 spacy-loggers-1.0.1 thinc-8.0.13\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/TL/.pyenv/versions/3.8.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "\u001b[K     |█████████████████████████▏      | 612.6 MB 346 kB/s eta 0:07:56     |█████████████▋                  | 331.7 MB 659 kB/s eta 0:11:16^C\n",
      "\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/TL/.pyenv/versions/3.8.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run only if you do not have SpaCy and the en_core_web_lg model installed on your device yet\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inappropriate-meditation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qp/v1yy0hns3vqdxskbv64ynv7c0000gn/T/ipykernel_84564/3019841561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxGLeCyxk-xH",
   "metadata": {
    "id": "sxGLeCyxk-xH"
   },
   "source": [
    "### Parts-of-speech (POS) tagging and dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zMsfimrOk-xH",
   "metadata": {
    "id": "zMsfimrOk-xH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BmAq17V-k-xI",
   "metadata": {
    "id": "BmAq17V-k-xI"
   },
   "outputs": [],
   "source": [
    "# Import spacy and the en_core_web_lg model\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except:\n",
    "    print(\"Error loading 'en_core_web_lg' model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0HPjvHgk-xI",
   "metadata": {
    "id": "r0HPjvHgk-xI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the en_core_web_lg_model to pre-process our text\n",
    "SpaCy's pre-trained model automatically determines various linguistic properties\n",
    "such as POS tags and dependency trees\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"The Philippine flight was delayed due to trouble with the airplane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HyIvDdZqk-xI",
   "metadata": {
    "id": "HyIvDdZqk-xI"
   },
   "outputs": [],
   "source": [
    "# Visualize every \"token\" in the document\n",
    "tokens = pd.DataFrame(columns=\n",
    "                      ['text', 'lemma', 'pos', 'tag',\n",
    "                       'dependency', 'shape', 'is_alphabet',\n",
    "                       'is_stopword', 'head_text', 'head_pos'])\n",
    "\n",
    "for token in doc:\n",
    "    data = [token.text, token.lemma_, token.pos_,\n",
    "            token.tag_, token.dep_, token.shape_,\n",
    "            token.is_alpha, token.is_stop,\n",
    "            token.head.text, token.head.pos_]\n",
    "    tokens.loc[len(tokens)] = data\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6LEWLj9rk-xI",
   "metadata": {
    "id": "6LEWLj9rk-xI"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_bL36cIGk-xI",
   "metadata": {
    "id": "_bL36cIGk-xI"
   },
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XG0ZpwCPk-xJ",
   "metadata": {
    "id": "XG0ZpwCPk-xJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the en_core_web_lg_model to pre-process our text\n",
    "SpaCy's pre-trained model also determines any named entities from text. \n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"The Philippine flight was delayed yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IP_JeOCdk-xJ",
   "metadata": {
    "id": "IP_JeOCdk-xJ"
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ktcZKj1k-xJ",
   "metadata": {
    "id": "3ktcZKj1k-xJ"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WIY8arb1k-xJ",
   "metadata": {
    "id": "WIY8arb1k-xJ"
   },
   "source": [
    "### Rules-based sentiment analysis\n",
    "\n",
    "For sentiment analysis, we will be using the VADER model ([source](https://github.com/cjhutto/vaderSentiment)). VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "\n",
    "We will modify the SpaCy pipeline to add the additional step of using the VADER model to calculate the sentiment polarity of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J30hyqNFk-xK",
   "metadata": {
    "id": "J30hyqNFk-xK"
   },
   "outputs": [],
   "source": [
    "# # Uncomment and run only if you do not have VADER installed on your device yet\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cM0QeQHek-xK",
   "metadata": {
    "id": "cM0QeQHek-xK"
   },
   "outputs": [],
   "source": [
    "# Import VADER model\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Import Doc for extending the SpaCy pipeline\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zLR_kKaLk-xK",
   "metadata": {
    "id": "zLR_kKaLk-xK"
   },
   "outputs": [],
   "source": [
    "# Define sentiment analysis extensions\n",
    "\"\"\"\n",
    "You can define the extension as a regular Python function:\n",
    "def sentiment_analysis(doc):\n",
    "    return sia.polarity_scores(doc.text)\n",
    "\"\"\"\n",
    "# Or you can create an anonymous lambda functions\n",
    "sentiment_analysis = lambda doc: sia.polarity_scores(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hb42zhi2k-xK",
   "metadata": {
    "id": "hb42zhi2k-xK"
   },
   "outputs": [],
   "source": [
    "# Instantiate NLP pipeline and set extensions\n",
    "nlp_with_sentiment = spacy.load(\"en_core_web_lg\")\n",
    "Doc.set_extension(\"sentiment\", getter=sentiment_analysis, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wL8_UDrxk-xK",
   "metadata": {
    "id": "wL8_UDrxk-xK"
   },
   "outputs": [],
   "source": [
    "doc = nlp_with_sentiment(\"I love this school, but it's tiring sometimes!\")\n",
    "doc._.sentiment # Acccess custom properties using the \"._.\" operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBKM44Aqk-xK",
   "metadata": {
    "id": "JBKM44Aqk-xK"
   },
   "source": [
    "### Process multiple documents\n",
    "\n",
    "SpaCy pipelines have an optimazation for processing multiple texts all at once using the `nlp.pipe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jlwp4XCRk-xK",
   "metadata": {
    "id": "jlwp4XCRk-xK"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I love this school, but it's tiring sometimes!\",\n",
    "        \"I'm looking forward to Christmas break.\",\n",
    "        \"I'm sad that midterms are over.\",\n",
    "        \"School is almost over.\",\n",
    "        \"I'll be relaxing during the Christmas break.\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GiL-lN4lk-xL",
   "metadata": {
    "id": "GiL-lN4lk-xL"
   },
   "outputs": [],
   "source": [
    "# Use the nlp.pipe() method to process a collection of texts\n",
    "docs = nlp_with_sentiment.pipe(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Opro5uXUk-xL",
   "metadata": {
    "id": "Opro5uXUk-xL"
   },
   "outputs": [],
   "source": [
    "df_modified = pd.DataFrame(columns=['text', 'document', 'com_sent', 'pos_sent', 'neg_sent', 'neu_sent'])\n",
    "for doc in docs:\n",
    "    df_modified = df_modified.append(\n",
    "        {\n",
    "            'text': doc.text,\n",
    "            'document': doc,\n",
    "            'com_sent': doc._.sentiment['compound'],\n",
    "            'pos_sent': doc._.sentiment['pos'],\n",
    "            'neg_sent': doc._.sentiment['neg'],\n",
    "            'neu_sent': doc._.sentiment['neu']\n",
    "        },\n",
    "        ignore_index=True\n",
    "    )\n",
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Er9ZOz9Mk-xL",
   "metadata": {
    "id": "Er9ZOz9Mk-xL"
   },
   "source": [
    "### Document embeddings\n",
    "\n",
    "SpaCy's `en_core_web_lg` model also comes with pre-trained document embeddings. As such, we can simply use them to immediately retrieve the embeddings of our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6H0YZ5vk-xL",
   "metadata": {
    "id": "p6H0YZ5vk-xL"
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vlRud-2Xk-xL",
   "metadata": {
    "id": "vlRud-2Xk-xL"
   },
   "outputs": [],
   "source": [
    "doc.vector.shape # The embedding uses 300 feature columns only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qbCv9PK9k-xL",
   "metadata": {
    "id": "qbCv9PK9k-xL"
   },
   "outputs": [],
   "source": [
    "doc.vector # This is the embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dk5tX8sAk-xM",
   "metadata": {
    "id": "dk5tX8sAk-xM"
   },
   "source": [
    "In the next following cells, we will attempt to embed a bunch of documents to retrieve their embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NUW2g0dEk-xM",
   "metadata": {
    "id": "NUW2g0dEk-xM"
   },
   "outputs": [],
   "source": [
    "df_embedded = df.copy()\n",
    "df_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-cNhv_ovk-xM",
   "metadata": {
    "id": "-cNhv_ovk-xM"
   },
   "outputs": [],
   "source": [
    "# Embed a bunch of documents and append them to a dataframe\n",
    "documents = []\n",
    "vectors = []\n",
    "\n",
    "for doc in nlp.pipe(df['text']):\n",
    "    documents.append(doc)\n",
    "    vectors.append(doc.vector)\n",
    "    \n",
    "df_embedded['document'] = pd.Series(documents, name='document')\n",
    "df_embedded = df_embedded.join(pd.DataFrame(vectors))\n",
    "df_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NO7G81Ark-xM",
   "metadata": {
    "id": "NO7G81Ark-xM"
   },
   "source": [
    "### Document similarity\n",
    "\n",
    "Now that we have document embeddings, we can use techniques such as cosine similarity to determine similarity between texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XX-yHDiJk-xM",
   "metadata": {
    "id": "XX-yHDiJk-xM"
   },
   "outputs": [],
   "source": [
    "a = nlp(\"Hello there!\")\n",
    "b = nlp(\"Greetings to you!\")\n",
    "c = nlp(\"Wikipedia is not a dictionary, or a usage or jargon guide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lRvv3XvHk-xM",
   "metadata": {
    "id": "lRvv3XvHk-xM"
   },
   "outputs": [],
   "source": [
    "a.similarity(b) # The two sentences look very similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGnxTlH0k-xN",
   "metadata": {
    "id": "lGnxTlH0k-xN"
   },
   "outputs": [],
   "source": [
    "a.similarity(c) # Not so similar (almost 50-50 coin toss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TSsCbYQdk-xN",
   "metadata": {
    "id": "TSsCbYQdk-xN"
   },
   "source": [
    "### Topic modeling and thematic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8GkeeE7Yk-xN",
   "metadata": {
    "id": "8GkeeE7Yk-xN"
   },
   "source": [
    "For topic modeling and thematic analysis, we will be using GenSim ([source](https://radimrehurek.com/gensim/)) and pyLDAvis ([source](https://github.com/bmabey/pyLDAvis)). We will use GenSim to implement the Latent-Dirichlet Allocation (LDA) model, and use pyLDAvis to visualize the results. \n",
    "\n",
    "WARNING: Your notebook might become buggy when using pyLDAvis. Once you are done looking at the visual, simply clear the output of the cell that uses the pyLDAvis chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pO1f8H77k-xN",
   "metadata": {
    "id": "pO1f8H77k-xN"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run only if you do not have gensim and pyLDAvis on your device yet\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rb8jAV6ck-xN",
   "metadata": {
    "id": "Rb8jAV6ck-xN"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis, pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "r1RT6CaEk-xN",
   "metadata": {
    "id": "r1RT6CaEk-xN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_modified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qp/v1yy0hns3vqdxskbv64ynv7c0000gn/T/ipykernel_84564/875504252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a corpus from the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplit_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_modified\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get tokens from each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Split_texts: List[ List[str] ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_texts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create a GenSimdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_modified' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a corpus from the documents\n",
    "split_texts = [[token.text for token in doc] for doc in df_modified['document']] # Get tokens from each document\n",
    "\n",
    "# Split_texts: List[ List[str] ]\n",
    "dictionary = Dictionary(split_texts) # Create a GenSimdictionary\n",
    "corpus = [dictionary.doc2bow(text) for text in split_texts] # Create a GenSim corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XAeeQmZEk-xN",
   "metadata": {
    "id": "XAeeQmZEk-xN"
   },
   "outputs": [],
   "source": [
    "# Create an LDA model with 3 topis from the generated corpus\n",
    "lda = LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=dictionary, \n",
    "    num_topics=3\n",
    ")\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zo_q3Upgk-xO",
   "metadata": {
    "id": "zo_q3Upgk-xO"
   },
   "outputs": [],
   "source": [
    "# Visualize the topic models and figure out the themes\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "vis\n",
    "\n",
    "# Right-click the cell then choose \"Clear outputs\" when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-blocking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "06_nlp_context_aware.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
